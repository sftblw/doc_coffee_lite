# DocCoffeeLite ‚òï

DocCoffee-lite is a EPUB in-place auto translation tool via LLMs. You put EPUB file, You get EPUB file.

99.99% vibe-coded, I don't know Elixir & Phoenix well. I whipped agent CLIs for translating phoenix books.

You will need LLM provider, even ollama with gpt-oss:20b will work, to use this repo.

This code base is currently minimal working status for actual use, without any auth, without any policy check, without any special ideas applied. I failed with that on first attempt. so, it's called "lite", and this is third attempt. First with python (ChatGPT Codex), second with phoenix + ash(Codex + Gemini CLI). this is third, with phoenix only - with oban (Gemini CLI).

and, I don't want to further improve this. this thing is purposed as temporary one... but it took my time a lot...

- First attempt failed because of mal-structured codebase, not depending on DB state and job queue.
- Second attempt failed because of ash framework complexity - somehow codes that generated by agent were not applied to DB state well, and my requirement is too complex to be understood by Codex and Gemini.
- This third attempt is phoenix only, with minimal featureset, ported from second attempt. and it... now works. phew.

Some considerations and decisions is made by me - parallel structuring, translation format, rolling context...

---

anyway, you'll need...

1. [mise](https://mise.jdx.dev/) for installing elixir and erlang BEAM. `mise install`.
2. then, `docker-compose up -d`
3. then `.env`
4. then `mix deps.get`, `mix ecto.setup` or `mix ecto.reset`, `mix phx.server`

---

below description is generated by Gemini CLI. I didn't touch anything.

---

DocCoffeeLite is a professional EPUB translation tool designed for high-quality, structurally-sound book translations using Large Language Models (LLMs). It features a highly resilient, batch-oriented translation engine capable of handling massive documents with narrative continuity.

## üöÄ Key Features

### üß† Intelligent Translation Engine
- **Atomic Batch Translation**: Automatically groups translation units based on character limits and unit counts to maximize LLM throughput while maintaining context.
- **Rolling Context Summary**: Passes a continuous narrative "baton" between batches. The LLM summarizes each batch to guide the next, ensuring consistency in characters, tone, and plot.
- **Semantic Placeholder System**: Uses specialized tags like `[[p_1]]` or `[[h1_2]]` to protect EPUB/HTML structure while giving the LLM explicit context about the element's role.

### üõ°Ô∏è Robustness & Reliability
- **Structured Output Enforcement**: Utilizes native JSON Schema (Structured Outputs) to guarantee valid data formats from the LLM.
- **Self-Healing Feedback Loop**: Automatically detects malformed responses or missing tags and provides specific feedback to the LLM for instant correction.
- **Crash Resilience**: Background workers are designed to recover automatically from server restarts or crashes, resuming exactly where they left off.

### ‚ö° Performance & Scalability
- **Smart Load Balancing**: Managed LLM pool (`LlmPool`) that intelligently distributes requests across multiple GPU nodes (e.g., Mac Studio, DGX Spark) with health tracking.
- **Concurrent Processing**: Leverages Oban for reliable, parallel background job execution.
- **Real-time UI**: Phoenix LiveView-powered dashboard with live progress counts, recent activity logs, and dynamic ETA calculation.

## üõ†Ô∏è Setup

### Prerequisites
- Elixir 1.15+ & Erlang/OTP 26+
- PostgreSQL
- LLM Provider (Ollama or OpenAI-compatible API)

### Configuration
Create a `.env` file in the project root:
```bash
# LLM Server(s) - Comma-separated for load balancing
LIVE_LLM_SERVER="http://192.168.1.10:11434,http://192.168.1.11:11434"
LIVE_LLM_MODEL="gpt-oss:20b"

# Worker Concurrency
OBAN_CONCURRENCY=3
```

### Installation
1. Install dependencies: `mix deps.get`
2. Setup database: `mix ecto.setup`
3. Start server: `mix phx.server`

Visit `http://localhost:4000` to start your first project.

## üìñ Usage Flow
1. **Create Project**: Upload your `.epub` file and select target language.
2. **Prepare**: The system granularly decomposes the book into translation units and generates translation policies.
3. **Configure LLM**: Set up your model endpoints (via `.env` or UI).
4. **Translate**: Click "Start". Monitor real-time progress and live translation snippets.
5. **Export**: Once 100% complete, download your translated EPUB with original formatting perfectly preserved.

---
Built with Elixir, Phoenix, and ‚ù§Ô∏è by sftblw.
