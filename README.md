# DocCoffeeLite â˜•

DocCoffee-lite is a EPUB in-place auto translation tool via LLMs. You put EPUB file, You get EPUB file.

99.99% vibe-coded, I don't know Elixir & Phoenix well. I whipped agent CLIs for translating phoenix books.

You will need LLM provider, even ollama with gpt-oss:20b will work, to use this repo.

This code base is currently minimal working status for actual use, without any auth, without any policy check, without any special ideas applied. I failed with that on first attempt. so, it's called "lite", and this is third attempt. First with python (ChatGPT Codex), second with phoenix + ash(Codex + Gemini CLI). this is third, with phoenix only - with oban (Gemini CLI).

and, I don't want to further improve this. this thing is purposed as temporary one... but it took my time a lot...

- First attempt failed because of mal-structured codebase, not depending on DB state and job queue.
- Second attempt failed because of ash framework complexity - somehow codes that generated by agent were not applied to DB state well, and my requirement is too complex to be understood by Codex and Gemini.
- This third attempt is phoenix only, with minimal featureset, ported from second attempt. and it... now works. phew.

Some considerations and decisions is made by me - parallel structuring, translation format, rolling context...

(I took screenshots - It finally meets my requirements!)

---

anyway, you'll need...

1. [mise](https://mise.jdx.dev/) for installing elixir and erlang BEAM. `mise install`.
2. then, `docker-compose up -d`
3. then `.env`
4. then `mix deps.get`, `mix ecto.setup` or `mix ecto.reset`, `mix phx.server`

---

below description is generated by Gemini CLI. I didn't touch anything.

---

## ğŸ“¸ Preview

| Home | Project Detail | Translation Review |
| :---: | :---: | :---: |
| ![Home](screenshots/1.png) | ![Project Detail](screenshots/2.png) | ![Translation Review](screenshots/3.png) |

DocCoffeeLite is a professional EPUB translation tool designed for high-quality, structurally-sound book translations using Large Language Models (LLMs). It features a highly resilient, batch-oriented translation engine capable of handling massive documents with narrative continuity.

## ğŸš€ Key Features

### ğŸ§  Intelligent Translation Engine
- **Atomic Batch Translation**: Automatically groups translation units based on character limits and unit counts to maximize LLM throughput while maintaining context.
- **Rolling Context Summary**: Passes a continuous narrative "baton" between batches. The LLM summarizes each batch to guide the next, ensuring consistency in characters, tone, and plot.
- **Semantic Placeholder System**: Uses specialized tags like `[[p_1]]` or `[[h1_2]]` to protect EPUB/HTML structure while giving the LLM explicit context about the element's role.

### ğŸ›¡ï¸ Robustness & Reliability
- **Structured Output Enforcement**: Utilizes native JSON Schema (Structured Outputs) to guarantee valid data formats from the LLM.
- **Self-Healing Feedback Loop**: Automatically detects malformed responses or missing tags and provides specific feedback to the LLM for instant correction.
- **Crash Resilience**: Background workers are designed to recover automatically from server restarts or crashes, resuming exactly where they left off.

### âš¡ Performance & Scalability
- **Smart Load Balancing**: Managed LLM pool (`LlmPool`) that intelligently distributes requests across multiple GPU nodes (e.g., Mac Studio, DGX Spark) with health tracking.
- **Concurrent Processing**: Leverages Oban for reliable, parallel background job execution.
- **Real-time UI**: Phoenix LiveView-powered dashboard with live progress counts, recent activity logs, and dynamic ETA calculation.

## ğŸ› ï¸ Setup

### Prerequisites
- Elixir 1.15+ & Erlang/OTP 26+
- PostgreSQL
- LLM Provider (Ollama or OpenAI-compatible API)

### Configuration
Create a `.env` file in the project root:
```bash
# LLM Server(s) - Comma-separated for load balancing
LIVE_LLM_SERVER="http://192.168.1.10:11434,http://192.168.1.11:11434"
LIVE_LLM_MODEL="gpt-oss:20b"
LIVE_LLM_API_KEY="sk-or-..."

# Worker Concurrency
OBAN_CONCURRENCY=3
```

### Installation
1. Install dependencies: `mix deps.get`
2. Setup database: `mix ecto.setup`
3. Start server: `mix phx.server`

Visit `http://localhost:4000` to start your first project.

## ğŸ“– Usage Flow
1. **Create Project**: Upload your `.epub` file and select target language.
2. **Prepare**: The system granularly decomposes the book into translation units and generates translation policies.
3. **Configure LLM**: Set up your model endpoints (via `.env` or UI).
4. **Translate**: Click "Start". Monitor real-time progress and live translation snippets.
5. **Export**: Once 100% complete, download your translated EPUB with original formatting perfectly preserved.

## âœ¨ Feature Deep Dive

### ğŸ§  Translation Continuity (Rolling Context)
DocCoffeeLite doesn't just translate sentence by sentence. It processes text in **smart batches**.
- **Context Awareness**: The summary of the previous batch is passed to the LLM for the next one. This ensures that character names, tone, and plot points remain consistent throughout the book.
- **Smart Grouping**: It automatically groups sentences into logical chunks (up to a token limit) to maximize LLM performance.

### ğŸ›¡ï¸ Quality Assurance Systems
- **Auto-Healing**: If an LLM returns broken JSON or missing tags, the system **automatically detects it** and retries with specific error feedback, fixing most issues without human intervention.
- **Similarity Guard**: Sometimes LLMs get lazy and just copy the source text. The **Similarity Guard** detects if the translation is too similar to the source (e.g., >90% match) and automatically flags it as "Dirty" for re-translation or review.

### ğŸ“ Review & Editing
- **Dirty Filter**: Use the "Dirty Only" filter in the translation view to quickly find and fix problematic units (failed translations, high similarity, or manual flags).
- **Bulk Actions**: You can perform Find & Replace operations across the entire project or mark filtered results for re-translation in bulk.

### ğŸ“¦ Import / Export
- **Migration Friendly**: Easily move your project to another server or back it up.
- **JSON + Source**: The export bundle includes all your translation data (metadata, progress) and the original source file, ensuring a perfect restore.
- **Offline Editing**: You can export the project, edit the `doc_coffee.json` manually if needed, and import it back (experimental).

### ğŸ“š Glossary & Policies
- **Glossary**: Define specific terms (Character names, Locations) that must be translated consistently.
- **Policies**: Set high-level rules (e.g., "Translate strictly", "Use honorifics") to guide the LLM's style.

---
Built with Elixir, Phoenix, and â¤ï¸ by sftblw.
